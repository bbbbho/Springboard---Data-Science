{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MovieLens Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendation System is important as it personalizes users' experience on the web. It captures users' pattern, preference, similiarties among users, among items and other patterns in order to help their customers to choose products more efficiently while increasing sales, which serves win-win strategy\n",
    "\n",
    "MovieLens dataset is a classic dataset for training recommendation models. It can be obtained from the GroupLens website. There are various datasets, but I will be using dataset that consists of 100,000 movie ratings by users (on a 1-5 scale). The main data file consists of a tab-separated list with user-id (starting at 1), item-id (starting at 1), rating, and timestamp as the four fields. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does Recommendation System work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 most popular types of recommender systems: Content-Based and Collaborative Filtering (CF). \n",
    "\n",
    "<b>Collaborative filtering</b> produces recommendations based on the knowledge of users’ attitude to items, that is it uses the “wisdom of the crowd” to recommend items. It searches other users and finds a smaller set with similar preference to the user. \n",
    "\n",
    "The algorithm has a very interesting property of being able to do feature learning on its own, which means that it can start to learn for itself what features to use. CF can be divided into <u>Memory-Based Collaborative Filtering</u> and <u>Model-Based Collaborative filtering</u>.\n",
    "\n",
    "<u>Memory-Based Collaborative Filtering</u> can divided into <i>user-item filtering</i> and <i>item-item filtering</i>. \n",
    "\n",
    "- A user-item filtering takes a particular user, find users that are similar to that user based on similarity of ratings, and recommend items that those similar users liked. \n",
    "    - <i> User-Item Collaborative Filtering: “Users who are similar to you also liked …”</i>\n",
    "- item-item filtering takes an item, find users who liked that item, and find other items that those users or similar users also liked. It takes items and outputs other items as recommendations.\n",
    "    - <i>Item-Item Collaborative Filtering: “Users who liked this item also liked …”</i>\n",
    "\n",
    "\n",
    "<u>Model-Based Collaborative filtering</u> is based on matrix factorization (MF) which has received greater exposure, mainly as an unsupervised learning method for latent variable decomposition and dimensionality reduction. This is also one of the most important class of techniques for winning the Netflix Prize. In their 2008 Progress Prize paper, Bell and Koren write\n",
    "\n",
    "<blockquote> <i>It seems that models based on matrix-factorization were found to be most accurate (and thus popular), as evident by recent publications and discussions on the Netflix Prize forum. We definitely agree to that, and would like to add that those matrix-factorization models also offer the important flexibility needed for modeling temporal effects and the binary view. Nonetheless, neighborhood models, which have been dominating most of the collaborative filtering literature, are still expected to be popular due to their practical characteristics - being able to handle new users/ratings without re-training and offering direct explanations to the recommendations.</i> </blockquote>\n",
    "\n",
    "Matrix factorization is widely used for recommender systems where it can deal better with scalability and sparsity than Memory-based CF. The goal of MF is to learn the latent preferences of users and the latent attributes of items from known ratings (learn features that describe the characteristics of ratings) to then predict the unknown ratings through the dot product of the latent features of users and items.\n",
    "\n",
    "<b>Content-based recommender systems </b> focus on the attributes of the items and give you recommendations based on the similarity between them. It provides personalized recommendation by matching user’s interests with description and attributes of items. \n",
    "Content-based techniques mostly analyze item features that were automatically extracted by information\n",
    "retrieval methods. \n",
    "\n",
    "Recommendations based on content-based techniques tend to overspecialize, because only items with a high similarity to those already rated will be suggested to the individual user. Another problem with content based\n",
    "recommenders is that a user first has to rate a sufficient number of items before the system is able to make accurate recommendations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MovieLens Recommendation System Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from scipy.sparse.linalg import svds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadinputData(input_file,test_size=0.25):\n",
    "\n",
    "    header = ['userId', 'movieId', 'rating', 'timestamp']\n",
    "    input_data = pd.read_csv(input_file,sep='\\t', names=header)\n",
    "    print('>> %s Moving Ratings Loaded' %len(input_data.index))\n",
    "\n",
    "    n_users = input_data.userId.unique().shape[0]\n",
    "    n_items = input_data.movieId.unique().shape[0]\n",
    "    print('>> Number of users = ' + str(n_users) + ' | Number of movies = ' + str(n_items))\n",
    "\n",
    "    train_data, test_data = train_test_split(input_data, test_size=test_size)\n",
    "    print('>> Training Size : %s Testing Size: %s' %(train_data.size, test_data.size))\n",
    "\n",
    "    # training and testing user-movie matrix\n",
    "    train_data_matrix = np.zeros((n_users, n_items))\n",
    "    for row in train_data.itertuples():\n",
    "        train_data_matrix[row[1] - 1, row[2] - 1] = row[3]\n",
    "    print('>> Training Matrix Created')\n",
    "\n",
    "    test_data_matrix = np.zeros((n_users, n_items))\n",
    "    for row in test_data.itertuples():\n",
    "        test_data_matrix[row[1] - 1, row[2] - 1] = row[3]\n",
    "    print('>> Testing Matrix Created')\n",
    "    \n",
    "    return train_data_matrix, test_data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 100000 Moving Ratings Loaded\n",
      ">> Number of users = 943 | Number of movies = 1682\n",
      ">> Training Size : 300000 Testing Size: 100000\n",
      ">> Training Matrix Created\n",
      ">> Testing Matrix Created\n"
     ]
    }
   ],
   "source": [
    " training, testing = loadinputData('../data/input/ml-100k/u.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  4. ...,  0.  0.  0.]\n",
      " [ 4.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 5.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  5.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first load in the data (path) as paramter and split the set into a training set and test set.\n",
    "The purpose of splitting into training and test set is to have our models trained with a defined set of data and to make predicted rating based on the  user and movie from the test set.\n",
    "\n",
    "As our test already contains the true rating from user to certain movies. This will allow us to evaluate our models accuracy by comparing predicted rating and true rating.\n",
    "\n",
    "As seen from the code, we are turning the data into matrix struture. This will create a user-item matrix where user as row and movie as column. The intercept of movie and user is the rating that the user gives to the movie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 4.73%\n"
     ]
    }
   ],
   "source": [
    "sparsity = float(len(training.nonzero()[0]))\n",
    "sparsity /= (training.shape[0] * training.shape[1])\n",
    "sparsity *= 100\n",
    "print ('Sparsity: {:4.2f}%'.format(sparsity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that 4.73% of the user-item ratings have a value, which as you can tell from the training set. The cells with 0 means empty entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation: performance criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance evaluation of recommendation systems is an entire topic all in itself. Some of the options include:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RMSE: $\\sqrt{\\frac{\\sum(\\hat y - y)^2}{n}}$\n",
    "- Precision / Recall / F-scores\n",
    "- ROC curves\n",
    "- Cost curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Root Mean Squared Error (RMSE)</b> is one of the most popular metric used to evaluate accuracy of predicted ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the mean_square_error (MSE) function from sklearn, where the RMSE is just the square root of MSE. \n",
    "As I only care about the predicted ratings that are in the test dataset, I filter out all other elements in the prediction matrix with prediction[ground_truth.nonzero()] (meaning the non-zero numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RMSE(prediction, truth, model_name='', print_=False):\n",
    "\n",
    "    # get the indices of test set with ratings, flatten to a list\n",
    "    prediction = prediction[truth.nonzero()].flatten()\n",
    "    ground_truth = truth[truth.nonzero()].flatten()\n",
    "    rmse = sqrt(mean_squared_error(prediction, ground_truth))\n",
    "    \n",
    "    if print_:\n",
    "        print('>> RMSE(%s): %s' %(model_name,rmse))\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can pass in the matrix from the Prediction Model with the test data into RMSE function to calculate the RMSE.\n",
    "RMSE is basically finding sample standard deviation of difference between the total difference predicted (y-hat) and true value (y). The difference are called residuals or prediction error. \n",
    "Our goal is to minimize the predicition error, meaning RMSE = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Model - Content-based filtering using mean ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MeanPredict(train_matrix, kind='user'):\n",
    "    zero_matrix = np.zeros(shape=(train_matrix.shape))\n",
    "\n",
    "    if kind == 'user':\n",
    "        mean_user_rating = train_matrix.mean(axis=1)\n",
    "        for i in range(len(zero_matrix)):\n",
    "            zero_matrix[i] = mean_user_rating[i]\n",
    "    if kind == 'item':\n",
    "        mean_item_rating = train_matrix.mean(axis=0)\n",
    "        for i in range(len(zero_matrix)):\n",
    "            zero_matrix[:,i] = mean_item_rating[i]\n",
    "\n",
    "    pred = zero_matrix\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an simple method by taking the mean of user or item (depends on the kind you choose). When kind = user, this means if user A rated 5 movies with sore - 5,3,4,4,5. If we are given the 6th movie, the model will predict the user A rating after watching the 6th by the mean of the previous ratings (21/5) = 4.25\n",
    "\n",
    "When kind = item, the concept is the same but instead of the mean of the users' previous rating, we use the previous rating of this movie to predict what rating it will get from the next user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is not supposed to be useful as a model because you can have problems of New user problem, New item problem, Data sparsity and other problems. This is a benchmark model because of how simple and unrealistic that the average of historical rating can be applied as the prediction. If our new models generate evaluation scores worse than this mean prediction model, it suggest our new models is worse than taking an average and need to be further improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> RMSE(User Mean Model): 3.4205435534281587\n",
      ">> RMSE(Item User Mean Model): 3.2364818273076525\n"
     ]
    }
   ],
   "source": [
    "item_pred = MeanPredict(training, kind='item')\n",
    "user_pred = MeanPredict(training, kind='user')\n",
    "RMSE(user_pred, testing,print_=True,model_name='User Mean Model')\n",
    "RMSE(item_pred, testing,print_=True,model_name='Item User Mean Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have scores bewteen 3.23 and 3.42 . This means our models should return a score less than 3.23 to be considered potentially valuable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<bold>Collaborative filtering models</bold> which can be split into two classes: user- and item-based collaborative filtering. In either scenario, one builds a similarity matrix. \n",
    "\n",
    "For user-based collaborative filtering, the user-similarity matrix will consist of some distance metric that measures the similarity between any two pairs of users. \n",
    "\n",
    "For item-similarity matrix will measure the similarity between any two pairs of items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common distance metric is <u>cosine similarity</u>. The metric can be thought of geometrically if one treats a given user's (item's) row (column) of the ratings matrix as a vector. For user-based collaborative filtering, two users' similarity is measured as the cosine of the angle between the two users' vectors. For users uu and u′u′, the cosine similarity is\n",
    "$$ sim(x,y) = \\frac{(x . y)}{\\sqrt{(x . x) (y . y)}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also other similarity function (distance function) such as eclidean, manhttan, pearson, jaccard and more. Each would have different properties but I will be using coisine similiarity here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using pairwise_distances function from sklearn to calculate the cosine similarity. It also contains other distance metrics if we want to change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarity(matrix, metrics, kind='user'):\n",
    "\n",
    "    # metrics options: [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’, ‘manhattan’]\n",
    "    if kind == 'user':\n",
    "        similarity = pairwise_distances(matrix, metric=metrics)\n",
    "    if kind == 'item':\n",
    "        similarity = pairwise_distances(matrix.T, metric=metrics)\n",
    "    print('>> Similarity Matrix Created')\n",
    "\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why we calculate similiaryt is to use it as the weight.\n",
    "\n",
    "For user-based collaborative filtering, we predict that a user A's rating for item 1 is given by the <u>weighted sum</u> of all other users' ratings for item 1 where the weighting is the cosine similarity between the each user and the input user A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We return a similarity matrix from this function is for our CFPredict function to get the dot product of similarity and other users' rating of the same product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing we can do is to <b>normalize</b> the rating.\n",
    "\n",
    "The idea here is that some users may tend always to give high or low ratings to all movies. The relative difference in the ratings that these users give is more important than the absolute values. To give an example: suppose, user A gives 4 stars to his favourite movies and 3 stars to all other good movies. Suppose now that another user B rates movies that he/she likes with 5 stars, and the movies he/she consider as average with 3 stars. These two users could have a very similar taste but treat the rating system differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CFPredict(train_matrix, kind='user', k=40, similarity_matrics = 'cosine'):\n",
    "\n",
    "    similiarty_matrix = similarity(train_matrix, kind=kind, metrics=similarity_matrics)\n",
    "\n",
    "    if kind == 'user':\n",
    "        mean_user_rating = train_matrix.mean(axis=1)\n",
    "        ratings_diff = (train_matrix - mean_user_rating[:, np.newaxis])\n",
    "        pred = similiarty_matrix.dot(ratings_diff) / np.array([np.abs(similiarty_matrix).sum(axis=1)]).T # normalization\n",
    "        pred += mean_user_rating[:, np.newaxis]\n",
    "\n",
    "    elif kind == 'item':\n",
    "        mean_movie_rating = train_matrix.mean(axis=0)\n",
    "        ratings_diff = (train_matrix - mean_movie_rating[np.newaxis, :])\n",
    "        pred = ratings_diff.dot(similiarty_matrix) / np.array([np.abs(similiarty_matrix).sum(axis=1)]) # normalization\n",
    "        pred += mean_movie_rating[np.newaxis, :]\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CFPredict is first calling 'similarity function' to get the similarity matrix. \n",
    "Then it looks for the kind of collabrative filter we prefer, then it gets the mean rating and rating difference.\n",
    "This is used for normalizing the rating.\n",
    "\n",
    "We then predict the the rating by using the line 'ratings_diff.dot(similiarty_matrix)'\n",
    "It return a matrix of rating prediction which can be used as input matrix for RMSE function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Similarity Matrix Created\n",
      ">> RMSE(User Collaborative Filtering): 3.1250136390594365\n",
      ">> Similarity Matrix Created\n",
      ">> RMSE(Item Collaborative Filtering): 3.115021235321692\n"
     ]
    }
   ],
   "source": [
    "user_prediction = CFPredict(training, kind='user')\n",
    "RMSE(user_prediction, testing, print_=True,model_name='User Collaborative Filtering')\n",
    "item_prediction = CFPredict(training, kind='item')\n",
    "RMSE(item_prediction, testing, print_=True,model_name='Item Collaborative Filtering')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE is slightly lower than Mean Model so it shows it is slightly better but not close to good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  SVD Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of MF is to learn the latent preferences of users and the latent attributes of items from known ratings (learn features that describe the characteristics of ratings) to then predict the unknown ratings through the dot product of the latent features of users and items. \n",
    "\n",
    "When you have a very sparse matrix, with a lot of dimensions, by doing matrix factorization you can restructure the user-item matrix into low-rank structure, and you can represent the matrix by the multiplication of two low-rank matrices, where the rows contain the latent vector. \n",
    "\n",
    "You fit this matrix to approximate your original matrix, as closely as possible, by multiplying the low-rank matrices together, which fills in the entries missing in the original matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the movielens data, we can consider latent vector as genre of the movie, user's gender, users' occupation and etc. These characteristics do not have to be provided but SVD will discover such characteristics. The model itself does not know what is the difference between 'horror movie' and 'comedy' nor knowing what the context of the genres. The general equation can be expressed as follows: X = U x S x VT\n",
    "\n",
    "Given an m x n matrix X:\n",
    "\n",
    " - U is an m x r orthogonal matrix\n",
    " - S is an r x r diagonal matrix with non-negative real numbers on the diagonal\n",
    " - VT is an r x n orthogonal matrix\n",
    "\n",
    "Matrix X can be factorized to U, S and V. The U matrix represents the feature vectors corresponding to the users in the hidden feature space and the V matrix represents the feature vectors corresponding to the items in the hidden feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SVD](../img/SVD.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def SVDPredict(train_matrix, max=20):\n",
    "\n",
    "    train_data, test_data = train_test_split(train_matrix, test_size=0.25)\n",
    "    best_score = 10\n",
    "    best_k = 1\n",
    "    for k in range(1, max):\n",
    "        u, s, vt = svds(train_data, k=k)\n",
    "        s_diag_matrix = np.diag(s)\n",
    "        X_pred_inner = np.dot(np.dot(u, s_diag_matrix), vt)\n",
    "        score = RMSE(X_pred_inner, test_data)\n",
    "        \n",
    "        if best_score < score:\n",
    "            best_score = score\n",
    "            best_k = k\n",
    "\n",
    "    u, s, vt = svds(train_matrix, k=best_k)\n",
    "    s_diag_matrix = np.diag(s)\n",
    "    X_pred = np.dot(np.dot(u, s_diag_matrix), vt)\n",
    "    print('>> %s latent factors' %best_k)\n",
    "    return X_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 1 latent factors\n",
      ">> RMSE(SVD): 2.982232763670124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.982232763670124"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = SVDPredict(training)\n",
    "RMSE(prediction,testing, print_=True,model_name='SVD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the SVDPredict function, there is a hyperparameter k that define how about latent factor should SVD model uses in order to predict. I have used cross-validation within the SVD to split the training data into training and evaluation data in order to find the best-k, which is the one that returns the lowest RMSE.\n",
    "\n",
    "The SVDPredict uses svds functino to split training data into 3 matrix - U ,S ,V.\n",
    "It is then used to multiply the matrix back to together to find the prediction value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD model brings the SVD to below 3 which is a good improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering Model & K Nearest Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve prediction MSE by only considering the top k users who are most similar to the input user (or, similarly, the top k items). That is, when we calculate the sums over top k most similar users' rating.\n",
    "\n",
    "This method is the the same method as the collaborative filtering model above but instead of looking at all other users and all other items, we pick the most similiar ones because they should be more representative.\n",
    "\n",
    "We also normalize the rating so we can ignore how user treat the rating system differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CFPredict_topk(ratings, kind='user', k=40,similarity_matrics='cosine'):\n",
    "\n",
    "    similiarty_matrix = similarity(ratings, kind=kind, metrics=similarity_matrics)\n",
    "\n",
    "    pred = np.zeros(ratings.shape)\n",
    "    if kind == 'user':\n",
    "        user_bias = ratings.mean(axis=1)\n",
    "        ratings = (ratings - user_bias[:, np.newaxis]).copy()\n",
    "        for i in range(ratings.shape[0]):\n",
    "            top_k_users = [np.argsort(similiarty_matrix[:, i])[:-k - 1:-1]] # extract the highest k rating from similarity matrix\n",
    "            for j in range(ratings.shape[1]):\n",
    "                pred[i, j] = similiarty_matrix[i, :][top_k_users].dot(ratings[:, j][top_k_users]) # extract top k similarity * top k users' rating\n",
    "                pred[i, j] /= np.sum(np.abs(similiarty_matrix[i, :][top_k_users]))\n",
    "        pred += user_bias[:, np.newaxis]\n",
    "    if kind == 'item':\n",
    "        item_bias = ratings.mean(axis=0)\n",
    "        ratings = (ratings - item_bias[np.newaxis, :]).copy()\n",
    "        for j in range(ratings.shape[1]):\n",
    "            top_k_items = [np.argsort(similiarty_matrix[:, j])[:-k - 1:-1]]\n",
    "            for i in range(ratings.shape[0]):\n",
    "                pred[i, j] = similiarty_matrix[j, :][top_k_items].dot(ratings[i, :][top_k_items].T)\n",
    "                pred[i, j] /= np.sum(np.abs(similiarty_matrix[j, :][top_k_items]))\n",
    "        pred += item_bias[np.newaxis, :]\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Similarity Matrix Created\n",
      ">> Similarity Matrix Created\n",
      ">> RMSE(User Collaborative Filtering + Top k): 3.393635018721629\n",
      ">> RMSE(Item Collaborative Filtering + Top k): 3.2173999874027466\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.2173999874027466"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prediction = CFPredict_topk(training, kind='user', k=10)\n",
    "item_prediction = CFPredict_topk(training, kind='item', k=10)\n",
    "RMSE(user_prediction, testing, print_=True,model_name='User Collaborative Filtering + Top k')\n",
    "RMSE(item_prediction, testing, print_=True,model_name='Item Collaborative Filtering + Top k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is suprising to see that if we add a KNN to Collaborative filtering, the result is worse than the CFPredict Model and just slightly better than the Mean Model.\n",
    "This suggests we can use other ways to find similar users/items. Using users' age, gender, occupation or movies' genre an year can be other ways to pick the best user/items for collaborative filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For SVD, We can minimize the squared error by applying alternating least square or stochastic gradient descent and uses regularization terms to prevent overfitting in future.\n",
    "- Use more advance SVD technique such as Asymmetric SVD and SVD++\n",
    "- Temporal Effects - take time as one of the factor of prediction.`\n",
    "- Ensemble Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
